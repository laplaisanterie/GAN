{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8. Disco_GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laplaisanterie/GAN/blob/master/3.%20Disco_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thBLxY-5HxzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4KIQs0qILBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "folder = \"colab/\"\n",
        "project_dir = \"disco\"\n",
        "\n",
        "base_path = Path(\"/content/gdrive/My Drive/\")\n",
        "project_path = base_path / folder / project_dir\n",
        "os.chdir(project_path)\n",
        "for x in list(project_path.glob(\"*\")):\n",
        "    if x.is_dir():\n",
        "        dir_name = str(x.relative_to(project_path))\n",
        "        os.rename(dir_name, dir_name.split(\" \", 1)[0])\n",
        "print(f\"현재 디렉토리 위치: {os.getcwd()}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL7BcYVAIfWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "from torch.autograd import Variable\n",
        "from itertools import chain\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "print('pytorch version: {}'.format(torch.__version__))\n",
        "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ77f_Ubw0Yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_variable(x):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRiC0_JdwHN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def denorm(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp(0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4u4RrO7wTb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GAN_Loss(input, target, criterion):\n",
        "    if target == True:\n",
        "        tmp_tensor = torch.FloatTensor(input.size()).fill_(1.0)\n",
        "        labels = Variable(tmp_tensor, requires_grad=False)\n",
        "    else:\n",
        "        tmp_tensor = torch.FloatTensor(input.size()).fill_(0.0)\n",
        "        labels = Variable(tmp_tensor, requires_grad=False)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        labels = labels.cuda()\n",
        "\n",
        "    return criterion(input, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDxc2-QfwVnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Feature_Loss(real_feats, fake_feats, criterion):\n",
        "    losses = 0\n",
        "    for real_feat, fake_feat in zip(real_feats, fake_feats):\n",
        "        l2 = (real_feat.mean(0) - fake_feat.mean(0)) * (real_feat.mean(0) - fake_feat.mean(0))\n",
        "        loss = criterion(l2, Variable(torch.ones(l2.size())).cuda())\n",
        "        losses += loss\n",
        "\n",
        "    return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxiBTSMDYOgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 200\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "batch_size = 8\n",
        "\n",
        "decay_gan_loss = 10000\n",
        "starting_rate = 0.01\n",
        "changed_rate = 0.5\n",
        "\n",
        "sample_path = './results'\n",
        "log_step = 10\n",
        "sample_step = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx7VV5BzSil6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize(64),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-mL7OYQoDvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageFolder(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.transformP = transforms.Compose([transforms.Resize((64,64)),\n",
        "                                             transforms.ToTensor(),\n",
        "                                             transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                                                  (0.5, 0.5, 0.5))])\n",
        "        \n",
        "        self.image_len = None\n",
        "\n",
        "        self.dir_base = './fruit_datasets'\n",
        "        \n",
        "        self.rootA = os.path.join(self.dir_base, 'Apple')\n",
        "        self.rootB = os.path.join(self.dir_base, 'Banana')\n",
        "        self.image_paths_A = list(map(lambda x: os.path.join(self.rootA, x), os.listdir(self.rootA)))\n",
        "        self.image_paths_B = list(map(lambda x: os.path.join(self.rootB, x), os.listdir(self.rootB)))\n",
        "        self.image_len = min(len(self.image_paths_A), len(self.image_paths_B))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        A_path = self.image_paths_A[index]\n",
        "        B_path = self.image_paths_B[index]\n",
        "        A = Image.open(A_path).convert('RGB')\n",
        "        B = Image.open(B_path).convert('RGB')\n",
        "\n",
        "        A = self.transformP(A)\n",
        "        B = self.transformP(B)\n",
        "\n",
        "        return {'A': A, 'B': B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.image_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hPItHvsS2f_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = ImageFolder()\n",
        "\n",
        "data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgJ2QSPgSYfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, extra_layers=False):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        if extra_layers == True: # For Car & Face DB\n",
        "            self.main = nn.Sequential(\n",
        "                # [-1, 3, 64x64] -> [-1, 64, 32x32]\n",
        "                nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                # [-1, 128, 16x16]\n",
        "                nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                # [-1, 256, 8x8]\n",
        "                nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                # [-1, 512, 4x4]\n",
        "                nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                # [-1, 100, 1x1]\n",
        "                nn.Conv2d(512, 100, 4, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(100),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                # [-1, 512, 4x4]\n",
        "                nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU(True),\n",
        "\n",
        "                # [-1, 256, 8x8]\n",
        "                nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.ReLU(True),\n",
        "\n",
        "                # [-1, 128, 16x16]\n",
        "                nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU(True),\n",
        "\n",
        "                # [-1, 64, 32x32]\n",
        "                nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(True),\n",
        "\n",
        "                # [-1, 3, 64x64]\n",
        "                nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
        "                nn.Tanh()\n",
        "            )\n",
        "\n",
        "        if extra_layers == False: # For Edges/Shoes/Handbags and Facescrub\n",
        "            self.main = nn.Sequential(\n",
        "                # [-1, 3, 64x64] -> [-1, 64, 32x32]\n",
        "                nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
        "                nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "                # [-1, 128, 16x16]\n",
        "                nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "                # [-1, 256, 8x8]\n",
        "                nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "                # [-1, 512, 4x4]\n",
        "                nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.LeakyReLU(0.2, inplace=False),\n",
        "\n",
        "                # [-1, 256, 8x8]\n",
        "                nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                # [-1, 128, 16x16]\n",
        "                nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                # [-1, 256, 32x32]\n",
        "                nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                # [-1, 3, 64x64]\n",
        "                nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
        "                nn.Tanh()\n",
        "            )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main( input )\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # [-1, 3, 64x64] -> [-1, 64, 32x32]\n",
        "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1, bias=False)\n",
        "        self.layer1 = nn.LeakyReLU(0.2, inplace=False)\n",
        "\n",
        "        # -> [-1, 128, 16x16]\n",
        "        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.layer2 = nn.LeakyReLU(0.2, inplace=False)\n",
        "\n",
        "        # -> [-1, 256, 8x8]\n",
        "        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.layer3 = nn.LeakyReLU(0.2, inplace=False)\n",
        "\n",
        "        # -> [-1, 512, 4x4]\n",
        "        self.conv4 = nn.Conv2d(256, 512, 4, 2, 1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.layer4 = nn.LeakyReLU(0.2, inplace=False)\n",
        "\n",
        "        # -> [-1, 1, 1x1]\n",
        "        self.conv5 = nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n",
        "        \n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input):\n",
        "        layer1 = self.layer1( self.conv1( input ) )\n",
        "        layer2 = self.layer2( self.bn2( self.conv2( layer1 ) ) )\n",
        "        layer3 = self.layer3( self.bn3( self.conv3( layer2 ) ) )\n",
        "        layer4 = self.layer4( self.bn4( self.conv4( layer3 ) ) )\n",
        "        layer5 = self.conv5(layer4)\n",
        "        \n",
        "        \n",
        "        x =self. sig(layer5)\n",
        "        \n",
        "        feature = [layer2, layer3, layer4]\n",
        "\n",
        "        return x, feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOwMxnaPa9Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_AtoB = Generator()\n",
        "generator_BtoA = Generator()\n",
        "discriminator_A = Discriminator()\n",
        "discriminator_B = Discriminator()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    generator_AtoB = generator_AtoB.cuda()\n",
        "    generator_BtoA = generator_BtoA.cuda()\n",
        "    discriminator_A = discriminator_A.cuda()\n",
        "    discriminator_B = discriminator_B.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGgn-VuuVZXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterionGAN = nn.BCELoss()\n",
        "criterionRecon = nn.MSELoss()\n",
        "criterionFeature = nn.HingeEmbeddingLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt6mkKm2WQ7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g_params = chain(generator_AtoB.parameters(), generator_BtoA.parameters())\n",
        "d_params = chain(discriminator_A.parameters(), discriminator_B.parameters())\n",
        "\n",
        "g_optimizer = torch.optim.Adam(g_params, lr, [beta1, beta2], weight_decay = 0.00001)\n",
        "d_optimizer = torch.optim.Adam(d_params, lr, [beta1, beta2], weight_decay = 0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tapB-qjZWaAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_step = len(data_loader) # For Print Log\n",
        "iter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, sample in enumerate(data_loader):\n",
        "        input_A = sample['A']\n",
        "        input_B = sample['B']\n",
        "        \n",
        "\n",
        "        # ===================== Random Shuffle =====================#\n",
        "        idx_A = np.arange(input_A.size(0))\n",
        "        idx_B = np.arange(input_B.size(0))\n",
        "        np.random.shuffle(idx_A)\n",
        "        np.random.shuffle(idx_B)\n",
        "\n",
        "        input_A = input_A.numpy()\n",
        "        input_B = input_B.numpy()\n",
        "\n",
        "        input_A = torch.from_numpy(input_A[idx_A])\n",
        "        input_B = torch.from_numpy(input_B[idx_B])\n",
        "\n",
        "        A = to_variable(input_A)\n",
        "        B = to_variable(input_B)\n",
        "\n",
        "        # ===================== Forward =====================#\n",
        "        generator_AtoB.zero_grad()\n",
        "        generator_BtoA.zero_grad()\n",
        "        discriminator_A.zero_grad()\n",
        "        discriminator_B.zero_grad()\n",
        "\n",
        "        A_to_B = generator_AtoB(A)\n",
        "        B_to_A = generator_BtoA(B)\n",
        "\n",
        "        A_to_B_to_A = generator_BtoA(A_to_B)\n",
        "        B_to_A_to_B = generator_AtoB(B_to_A)\n",
        "\n",
        "        A_real, A_real_features = discriminator_A(A)\n",
        "        A_fake, A_fake_features = discriminator_A(B_to_A)\n",
        "\n",
        "        B_real, B_real_features = discriminator_B(B)\n",
        "        B_fake, B_fake_features = discriminator_B(A_to_B)\n",
        "\n",
        "        # ===================== Train D =====================#\n",
        "        loss_D_A = (GAN_Loss(A_real, True, criterionGAN) + GAN_Loss(A_fake, False, criterionGAN)) * 0.5\n",
        "        loss_D_B = (GAN_Loss(B_real, True, criterionGAN) + GAN_Loss(B_fake, False, criterionGAN)) * 0.5\n",
        "        loss_D = loss_D_A + loss_D_B\n",
        "\n",
        "        # ===================== Train G =====================#\n",
        "        loss_G_Recon_A = criterionRecon(A_to_B_to_A, A)\n",
        "        loss_G_Recon_B = criterionRecon(B_to_A_to_B, B)\n",
        "\n",
        "        loss_G_A = GAN_Loss(A_fake, True, criterionGAN)\n",
        "        loss_G_B = GAN_Loss(B_fake, True, criterionGAN)\n",
        "\n",
        "        loss_G_A_feature = Feature_Loss(A_real_features, A_fake_features, criterionFeature)\n",
        "        loss_G_B_feature = Feature_Loss(B_real_features, B_fake_features, criterionFeature)\n",
        "\n",
        "        if iter < decay_gan_loss:\n",
        "            rate = starting_rate\n",
        "        else:\n",
        "            rate = changed_rate\n",
        "\n",
        "        loss_G_A_Total = (loss_G_A*0.1 + loss_G_A_feature*0.9) * (1.-rate) + loss_G_Recon_A * rate\n",
        "        loss_G_B_Total = (loss_G_B*0.1 + loss_G_B_feature*0.9) * (1.-rate) + loss_G_Recon_B * rate\n",
        "\n",
        "        loss_G = loss_G_A_Total + loss_G_B_Total\n",
        "\n",
        "        # ===================== Optimized =====================#\n",
        "\n",
        "        if iter % 3 == 0:\n",
        "            loss_D.backward()\n",
        "            d_optimizer.step()\n",
        "        else:\n",
        "            loss_G.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "        # print the log info\n",
        "        if (i + 1) % log_step == 0:\n",
        "            print('Iteration [%d], Epoch [%d/%d], BatchStep[%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
        "                  % (iter + 1, epoch + 1, num_epochs, i + 1, total_step, loss_D.item(), loss_G.item()))\n",
        "            \n",
        "        if (iter + 1) % sample_step == 0:\n",
        "            res1 = torch.cat((torch.cat((A, A_to_B), dim=2), A_to_B_to_A), dim=2)\n",
        "            res2 = torch.cat((torch.cat((B, B_to_A), dim=2), B_to_A_to_B), dim=2)\n",
        "            res = torch.cat((res1, res2), dim=2)\n",
        "            torchvision.utils.save_image(denorm(res.data), os.path.join(sample_path, 'Generated-%d-%d-%d.png' % (iter + 1, epoch + 1, i + 1)))\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "    # save the model parameters for each epoch\n",
        "#     g_pathAtoB = os.path.join(args.model_path, 'generatorAtoB-%d.pkl' % (epoch + 1))\n",
        "#     g_pathBtoA = os.path.join(args.model_path, 'generatorBtoA-%d.pkl' % (epoch + 1))\n",
        "#     torch.save(generator_AtoB.state_dict(), g_pathAtoB)\n",
        "#     torch.save(generator_BtoA.state_dict(), g_pathBtoA)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}